import json
import os
import re
import subprocess
import sys
import random
from tools.math_tool import generate_normal_random
from utils.chrome import is_docker, create_chrome_driver
from utils.logger import logger, setup_url_logger
from utils.config import config
from scrapy.crawler import CrawlerProcess
from selenium.webdriver.common.by import By
from scrapy.utils.project import get_project_settings
from trace_spider.spiders.trace import TraceSpider
import threading
import time
from traffic.capture import capture, stop_capture
from datetime import datetime
from utils.task import task_instance

duration = int(config["spider"]["duration"])
process = CrawlerProcess(get_project_settings())
crawlers_timer = None


# æ¸…é™¤æµè§ˆå™¨è¿›ç¨‹
def kill_chrome_processes():
    try:
        # Run the command to kill all processes containing 'chrome'
        logger.info(f"æ¸…ç†æµè§ˆå™¨è¿›ç¨‹")
        subprocess.run(['sudo', 'pkill', '-f', 'chrome'], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    except subprocess.CalledProcessError as e:
        print(f"Error occurred: {e.stderr.decode('utf-8')}")

# æ¸…ç†æµé‡æ•è·è¿›ç¨‹
def kill_tcpdump_processes():
    try:
        # Run the command to kill all processes containing 'chrome'
        logger.info(f"æ¸…ç†æµé‡æ•è·è¿›ç¨‹")
        subprocess.run(['sudo', 'pkill', '-f', 'tcpdump'], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    except subprocess.CalledProcessError as e:
        print(f"Error occurred: {e.stderr.decode('utf-8')}")


# æµé‡æ•è·è¿›ç¨‹
def traffic():
    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.now()
    # æ ¼å¼åŒ–è¾“å‡º
    formatted_time = current_time.strftime("%Y%m%d_%H%M%S")
    allowed_domain = task_instance.current_allowed_domain
    capture(allowed_domain, formatted_time, sys.argv[1:])


# åœæ­¢çˆ¬è™«
def stop_crawlers():
    logger.info("å®šæ—¶å™¨è§¦å‘ï¼Œåœæ­¢æ‰€æœ‰çˆ¬è™«")
    global crawlers_timer
    for crawler in process.crawlers:
        crawler.stop()
    crawlers_timer = None


# å¯åŠ¨å®šæ—¶å™¨
def stop_crawlers_after_delay():
    global crawlers_timer
    crawlers_timer = threading.Timer(duration, stop_crawlers)
    crawlers_timer.start()


# å–æ¶ˆå®šæ—¶å™¨
def cancel_timer():
    global crawlers_timer
    if crawlers_timer is not None:
        logger.info(f"çˆ¬è™«æå‰ç»“æŸï¼Œå…³é—­å®šæ—¶å™¨")
        crawlers_timer.cancel()


# å¯åŠ¨çˆ¬è™«
def start_spider():
    # æ·»åŠ ä½ è¦è¿è¡Œçš„çˆ¬è™«
    process.crawl(TraceSpider)

    logger.info(f"å¼€å§‹çˆ¬å–æ•°æ®")
    # å¼€å¯å®šæ—¶å™¨
    stop_crawlers_after_delay()
    # å¯åŠ¨çˆ¬è™«
    process.start()


def start_task():
    kill_chrome_processes()
    kill_tcpdump_processes()
    time.sleep(5)
    # å¼€æµé‡æ”¶é›†
    traffic_thread = threading.Thread(target=traffic)
    traffic_thread.start()

    dir_path = f'huggingface_data/Pots'
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
    file_path = os.path.join(dir_path, 'pots_web.json')
    browser = create_chrome_driver()
    browser.get("https://huggingface.co/posts")
    # 5ï¸âƒ£ ä½¿ç”¨ XPath å®šä½æŒ‰é’® (XPath æ–¹æ³•)
    button = browser.find_element(By.XPATH,
                                  '//button[contains(@class, "btn mx-4 w-48 text-smd") and contains(text(), "Load more")]')

    # 6ï¸âƒ£ ç¡®ä¿æŒ‰é’®åœ¨è§†é‡å†…
    browser.execute_script("arguments[0].scrollIntoView(true);", button)
    # 5ï¸âƒ£ ä½¿ç”¨ XPath æå–æ‰€æœ‰ class="relative" çš„HTMLå…ƒç´ 
    prelength = 0
    is_bottom = False
    while not is_bottom:
        try:
            elements = browser.find_elements(By.XPATH, '//*[@class="relative"]')
            print(f"ç¬¬{len(elements) / 10}æ¬¡æå–å†…å®¹ï¼Œæ•°é‡ä¸º{len(elements)}")
            for elenium in elements[-10:]:

                target_div = elenium.find_element(By.XPATH,
                                                  './/div[contains(@class, "relative overflow-hidden break-words px-4 text-smd/6")]')

                # 6ï¸âƒ£ æå–è¯¥ div ä¸­çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹ (åŒ…æ‹¬æ‰€æœ‰çš„åµŒå¥— span å’Œå­å…ƒç´ çš„æ–‡æœ¬)
                text_content = target_div.text
                cleaned_content = re.sub(r'\n\s*\n+', '\n', text_content)
                cleaned_content = "\n".join([line.strip() for line in cleaned_content.splitlines()])
                # print("ğŸ“˜ ä¸»è¦çš„æ–‡æœ¬å†…å®¹ï¼š\n", cleaned_content)

                # 7ï¸âƒ£ ä½¿ç”¨ XPath æå–è¯¥ div ä¸­çš„æ‰€æœ‰é“¾æ¥ (aæ ‡ç­¾)
                links = target_div.find_elements(By.XPATH, './/a[@href]')

                # 8ï¸âƒ£ éå†æ‰€æœ‰çš„é“¾æ¥ï¼Œè·å–é“¾æ¥çš„æ–‡æœ¬å’ŒURL
                extracted_links = []
                for link in links:
                    link_text = link.text
                    link_url = link.get_attribute('href')
                    extracted_links.append((link_text, link_url))

                # 9ï¸âƒ£ è¾“å‡ºæ‰€æœ‰é“¾æ¥çš„æ–‡æœ¬å’ŒURL
                # for i, (link_text, link_url) in enumerate(extracted_links, start=1):
                #     print(f"ğŸ”— é“¾æ¥ {i}:")
                #     print(f"  - é“¾æ¥æ–‡æœ¬: {link_text}")
                #     print(f"  - é“¾æ¥åœ°å€: {link_url}\n")

                # ğŸ”¥ ç»„ç»‡æ•°æ®
                data = {"content": cleaned_content, "links": extracted_links}

                # ğŸ”¥ ä¿å­˜ä¸ºJSONæ–‡ä»¶, 'w') as f:
                with open(file_path, 'a', encoding='utf-8') as f:
                    json.dump(data, f)
                    f.write("\n")

            # 7ï¸âƒ£ å•å‡»æŒ‰é’®
            button.click()
            print("Clicked 'Load more' button")
            random_integer = random.randint(5, 15)  # åŒ…æ‹¬1å’Œ10
            time.sleep(generate_normal_random() + random_integer)
            elements = browser.find_elements(By.XPATH, '//*[@class="relative"]')
            if len(elements) == prelength:
                attemp_index = 0
                for i in range(5):
                    print("å†æ¬¡å°è¯•ç‚¹å‡»æŒ‰é’®")
                    # 7ï¸âƒ£ å•å‡»æŒ‰é’®
                    button.click()
                    print("Clicked 'Load more' button")
                    random_integer = random.randint(5, 15)  # åŒ…æ‹¬1å’Œ10
                    time.sleep(generate_normal_random() + random_integer)
                    elements = browser.find_elements(By.XPATH, '//*[@class="relative"]')
                    if len(elements) == prelength:
                        attemp_index += 1

                if attemp_index == 5:
                    is_bottom = True
            prelength = len(elements)


        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")


    logger.info(f"çˆ¬å–æ•°æ®ç»“æŸ, ç­‰å¾…10ç§’.è®©æµè§ˆå™¨åŠ è½½å®Œæ‰€æœ‰å·²è¯·æ±‚çš„é¡µé¢")
    time.sleep(10)


    kill_chrome_processes()
    logger.info(f"ç­‰å¾…TCPç»“æŸæŒ¥æ‰‹å®Œæˆ")
    # time.sleep(60)

    # å…³æµé‡æ”¶é›†
    logger.info(f"å…³æµé‡æ”¶é›†")
    stop_capture()

    logger.info(f"{task_instance.current_start_url}æµé‡æ”¶é›†ç»“æŸï¼Œå…±çˆ¬å–{task_instance.requesturlNum}ä¸ªé¡µé¢")
    kill_tcpdump_processes()
    cancel_timer()


if __name__ == "__main__":
    if is_docker():
        start_task()
    else:
        start_spider()
